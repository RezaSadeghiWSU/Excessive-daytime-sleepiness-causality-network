{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a6ddc9-43bb-455c-805a-594641b0f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: June 11, 2025\n",
    "# Author: Lydia Bullock\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Note - final function for imputing columns is called replace_nans. This function uses all other functions found above it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868ca9c1-94a1-4a83-86af-58e10f3d7d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adecf282-4085-4baa-a522-af5de876812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the NaNs and determine what percentage of the data is NaN for each column of the dataframe\n",
    "\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "        \n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    \n",
    "    # credit for creating the table: https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction. \n",
    "    \n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "    # Sort the table by percentage of missing ascending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values').round(1)\n",
    "\n",
    "    mis_val_table_ren_columns.index.name = 'Columns'\n",
    "\n",
    "    return mis_val_table_ren_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c0b9f2-fdbc-4324-901b-9c22d2c12fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the columns in the data frame that do not have enough information, except for the reference column. \n",
    "#If threshold = 30, this will remove all columns that have less than 70% data (30% of the data are NaNs).\n",
    "\n",
    "def remove_columns(df, threshold):\n",
    "\n",
    "    percent_missing_values = missing_values_table(df)\n",
    "    cols_to_drop = []\n",
    "\n",
    "    # Remove all columns in which there are more than threshold % of NaNs (not enough data, too many NaNs)\n",
    "    for col_name, percent in percent_missing_values['% of Total Values'].items():\n",
    "        if percent > threshold:\n",
    "            print(f\"Column \\\"{col_name}\\\" exceeds the threshold.\")\n",
    "            cols_to_drop.append(col_name)\n",
    "    \n",
    "    # Drop them all at once\n",
    "    df_cleaned = df.drop(columns=cols_to_drop) #the drop function does not modify the original dataframe unless inplace=True\n",
    "    \n",
    "    return df_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39bff57-1374-4d5d-9235-ee5fe79f4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a histogram to visualize how much data is in each column\n",
    "\n",
    "def hist_missing_values(df, threshold, save_plot=False): #threshold is percentage of missing data - if missing data is higher than threshold, bad\n",
    "\n",
    "    df_missing_values = missing_values_table(df)\n",
    "    \n",
    "    #color coding the threshold\n",
    "    colors = []\n",
    "    for percent in df_missing_values['% of Total Values']:\n",
    "        if percent > threshold:\n",
    "            colors.append('red')\n",
    "        else: \n",
    "            colors.append('blue')\n",
    "            \n",
    "    # Percentage of data that is missing\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(df_missing_values.index, df_missing_values['% of Total Values'], color = colors, width=1) \n",
    "    #df_missing_values.index is the name of the column/category and df_missing_values['% of Total Values'] is the percentage of nans in each category\n",
    "\n",
    "    # Add dotted line at threshold\n",
    "    plt.axhline(y=threshold, color='black', linestyle='dotted', linewidth=1)\n",
    "    \n",
    "    # Labeling \n",
    "    plt.ylabel('% of Total Values')\n",
    "    plt.xlabel('Category')\n",
    "    plt.title('Percentage of Missing Values per Column')\n",
    "    if len(df_missing_values.index) > 50:\n",
    "        plt.xticks([])\n",
    "    else: \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    if save_plot:\n",
    "        plt.savefig('Missing_Values.png', dpi = 300)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Percentage of data that exists\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(df_missing_values.index, 100-df_missing_values['% of Total Values'], color = colors, width=1) \n",
    "    #df_missing_values.index is the name of the column/category and df_missing_values['% of Total Values'] is the percentage of nans in each category\n",
    "\n",
    "    # Add dotted line at threshold\n",
    "    plt.axhline(y=100-threshold, color='black', linestyle='dotted', linewidth=1)\n",
    "    \n",
    "    # Labeling\n",
    "    plt.ylabel('% of Total Values')\n",
    "    plt.xlabel('Category')\n",
    "    plt.title('Percentage of Existing Data per Column')\n",
    "    if len(df_missing_values.index) > 50:\n",
    "        plt.xticks([])\n",
    "    else: \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    if save_plot:\n",
    "        plt.savefig('Existing_Data.png', dpi = 300)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e8e2fb-688d-43ad-8875-595df8f34cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate columns with more than 70% of data available (less than 30% nans)\n",
    "#find the location of the nans within that column (the row number)\n",
    "#find the values in the reference column that corresponds to the nans\n",
    "\n",
    "def locate_values(df, threshold, reference_column_name):\n",
    "\n",
    "    #dataframe of percentage of missing values\n",
    "    df_missing_values = missing_values_table(df)\n",
    "\n",
    "    # Locate and list the values corresponding to the NaNs \n",
    "    # Result is a dataframe\n",
    "    \n",
    "    df_combined_rows = [] #all rows of the original data frame\n",
    "    relevant_rows = [] #only contains rows in which there are nans that pass the threshold % and is not the reference column\n",
    "     \n",
    "    for col_name, row in df_missing_values.iterrows():\n",
    "        num_nans = row['Missing Values']\n",
    "        percent_missing = row['% of Total Values']\n",
    "        \n",
    "        # Isolate columns with more than 70% of data available (less than 30% nans)\n",
    "        if percent_missing <= threshold and col_name != reference_column_name:\n",
    "\n",
    "            # Find the location of the nans within that column (the row number)\n",
    "            nan_index = df[df[col_name].isna()].index\n",
    "\n",
    "            # Find the values in the reference column that correspond to the nans\n",
    "            reference_column_values = df.loc[nan_index, reference_column_name].values\n",
    "            \n",
    "            # Append relevant row data and combined row data: column name, NaN index, and list of values\n",
    "            df_combined_rows.append({\n",
    "                'Columns': col_name,\n",
    "                'Row Number': nan_index.values,\n",
    "                f'Values in \\\"{reference_column_name}\\\" Column Corresponding to NaNs': reference_column_values\n",
    "            })\n",
    "            \n",
    "            relevant_rows.append({\n",
    "                'Columns': col_name,\n",
    "                'Row Number': nan_index.values,\n",
    "                f'Values in \\\"{reference_column_name}\\\" Column Corresponding to NaNs': reference_column_values\n",
    "            })\n",
    "        \n",
    "        elif percent_missing > threshold and col_name != reference_column_name: \n",
    "            print(f\"Column \\\"{col_name}\\\" exceeds the threshold.\")\n",
    "            \n",
    "        elif col_name == reference_column_name:\n",
    "            print(f\"Column \\\"{col_name}\\\" is the reference column.\")\n",
    "        \n",
    "        else:\n",
    "            print('Something is happening here. IDK.')\n",
    "    \n",
    "    new_info = pd.DataFrame(df_combined_rows)\n",
    "    new_info = new_info.set_index('Columns')\n",
    "    df_combined = pd.concat([df_missing_values, new_info], axis=1)\n",
    "\n",
    "    result_df = pd.DataFrame(relevant_rows)\n",
    "    result_df = result_df.set_index('Columns')\n",
    "\n",
    "    return result_df, df_combined\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb14e679-8ae7-4f0d-aeb7-8a1b9f3bd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate the other values within the reference column that match the value corresponding to the NaN\n",
    "\n",
    "def find_matching_values(df, threshold, reference_column_name):\n",
    "\n",
    "    reference_column_values, combined = locate_values(df, threshold, reference_column_name)\n",
    "\n",
    "    # From the table of corresponding NaN values, we must pair the index and the value together so that we can work through each pair individually.\n",
    "    # First iterate through each row of the whole table to extract the index and value\n",
    "    \n",
    "    matching_values_rows = []\n",
    "    relevant_rows = [] #only contains rows in which there are nans that pass the threshold % and is not the reference column\n",
    "    \n",
    "    for col_name, row in reference_column_values.iterrows():\n",
    "        row_num = row['Row Number']\n",
    "        corresponding_values = row[f'Values in \\\"{reference_column_name}\\\" Column Corresponding to NaNs']\n",
    "        pairs = list(zip(row_num, corresponding_values))\n",
    "\n",
    "        all_matching_values = []\n",
    "        # Second, loop through each index-value pair\n",
    "        for index, value in pairs:\n",
    "            \n",
    "            # Find the other values in the reference column that match the value we are on - below actually finds the entire row\n",
    "            matching_rows = df[(df[reference_column_name] == value) & (df.index != index)]\n",
    "            \n",
    "            # From the matching_rows dataframe, isolate the values in the column that we are on (col_name)\n",
    "            matching_values = matching_rows[col_name].values\n",
    "\n",
    "            all_matching_values.append(matching_values)\n",
    "    \n",
    "        matching_values_rows.append({'Columns': col_name,\n",
    "                                    f'Values in Column that Correspond to \\\"{reference_column_name}\\\" Value': all_matching_values}) \n",
    "        \n",
    "            \n",
    "    all_matching_values = pd.DataFrame(matching_values_rows)\n",
    "    all_matching_values = all_matching_values.set_index('Columns')\n",
    "    df_combined = pd.concat([combined, all_matching_values], axis=1)\n",
    "\n",
    "    return all_matching_values, df_combined\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f86e7c-9800-4271-a7e3-3e283c8c0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode or mean of the list of values that correspond to the NaN\n",
    "\n",
    "def find_mode_mean(df, threshold, reference_column_name):\n",
    "\n",
    "    matching_values, combined = find_matching_values(df, threshold, reference_column_name)\n",
    "\n",
    "    mode_mean_rows = []\n",
    "    for col_name, row in matching_values.iterrows():\n",
    "        values = row[f'Values in Column that Correspond to \\\"{reference_column_name}\\\" Value']\n",
    "\n",
    "        # Check the data type in the original column to determine if we need to use mean or mode:\n",
    "\n",
    "        # Ignore the null values because that is what we are replacing\n",
    "        column_values = df[col_name].dropna() \n",
    "\n",
    "        # Check if data type is binary (with special cases)\n",
    "        # -55 = I don't know (par_0800)\n",
    "        # -44 = I rarely/never skip lunch (diet_0600)\n",
    "        if np.isin(column_values.unique(), [0, 1, -55, -44]).all():\n",
    "            column_type = 'binary'    \n",
    "\n",
    "        # Check if data type is numeric. These columns are the scale scores that we couldn't bin/categorize\n",
    "        elif pd.api.types.is_numeric_dtype(column_values):\n",
    "            column_type = 'numeric'\n",
    "\n",
    "            # Check the decimal place of the entries (for rounding)\n",
    "            decimal_place = get_decimal_places(column_values)\n",
    "\n",
    "        # Categorical data type\n",
    "        else:\n",
    "            column_type = 'categorical'\n",
    "\n",
    "    \n",
    "        # Each array in values is a list a entries that match the entry in the reference column that corresponds to a NaN in another column\n",
    "        # There can be multiple NaNs in a column and therefore multiple lists of matching values, so that is why we iterate\n",
    "        \n",
    "        mode_mean = []\n",
    "        for array in values:\n",
    "            \n",
    "            # Remove NaNs\n",
    "            arr_clean = [x for x in array if pd.notna(x)]\n",
    "\n",
    "            # If the array is empty, append NaN and know that something went wrong\n",
    "            if len(arr_clean) == 0:\n",
    "                mode_mean.append(np.nan)\n",
    "                print('The matching values array should not be empty.')\n",
    "\n",
    "            # If binary or categorical, append the mode\n",
    "            elif column_type == 'binary' or column_type == 'categorical': \n",
    "                # If two or more values occur at the same frequency, just pick the first \n",
    "                mode_series = pd.Series(arr_clean).mode()\n",
    "                mode_mean.append(mode_series.iloc[0]) # mode() can return multiple modes, so pick the first one\n",
    "\n",
    "            # If numeric, append the rounded mean\n",
    "            elif column_type == 'numeric':\n",
    "                mean = np.mean(arr_clean)\n",
    "                rounded_mean = round_up(mean, decimal_place)\n",
    "                mode_mean.append(rounded_mean)\n",
    "\n",
    "        mode_mean_rows.append({'Columns': col_name,\n",
    "                                    'Mode/Mean': mode_mean})\n",
    "    \n",
    "    all_modes_means = pd.DataFrame(mode_mean_rows)\n",
    "    all_modes_means = all_modes_means.set_index('Columns')\n",
    "    df_combined = pd.concat([combined, all_modes_means], axis=1)\n",
    "\n",
    "\n",
    "    return all_modes_means, df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf0b328-71c0-41d5-a02b-8e39073fab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a column, determine to which decimal place the entries are rounded\n",
    "\n",
    "def get_decimal_places(column):\n",
    "   \n",
    "    decimals = []\n",
    "    for val in column.dropna():\n",
    "        if isinstance(val, float):\n",
    "            text = str(val)\n",
    "            if '.' in text:\n",
    "                decimals.append(len(text.rstrip('0').split('.')[-1])) #ignores trailing zeroes\n",
    "    \n",
    "    return max(decimals) if decimals else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b458a2-55b5-40a6-8153-d672082fb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick math to always round a number up\n",
    "\n",
    "def round_up(value, decimal_places):\n",
    "    \n",
    "    factor = 10 ** decimal_places\n",
    "    \n",
    "    return math.ceil(value * factor) / factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "607677bb-9dcb-4e86-989b-0f8d70083390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL IMPUTATION FUNCTION\n",
    "\n",
    "def replace_nans(df, threshold, reference_column_name):\n",
    "\n",
    "    #Get dfs with the mode/mean column included\n",
    "    all_modes_means, combined = find_mode_mean(df, threshold, reference_column_name)\n",
    "\n",
    "    # Get the dataframe of the original data (rows without ess_0900 removed and columns with over 30% NaNs removed)\n",
    "    removed_columns_df_copy = df.copy()\n",
    "\n",
    "    # Iterate through the whole table and pair up the mode/mean with the location of the original NaN\n",
    "    for col_name, row in all_modes_means.iterrows():\n",
    "        row_num = combined.loc[col_name]['Row Number']\n",
    "        replacements = row['Mode/Mean'] \n",
    "        pairs = list(zip(row_num, replacements))\n",
    "\n",
    "        # Replace the NaN with the mode/mean value\n",
    "        for index, value in pairs:\n",
    "            removed_columns_df_copy.loc[index, col_name] = value\n",
    "    \n",
    "    return removed_columns_df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182d3289-712d-4fe4-a3b5-86843d771abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the total ess score into categories\n",
    "\n",
    "def categorize_0900_scores(df, dataset_number):\n",
    "    \n",
    "    new_column = []\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Iterate through the rows of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #Depending on the range the value falls in and the dataset type, assign EDS categories to a new column\n",
    "\n",
    "        # The simple version - two categories - Normal or Sleepy\n",
    "        if dataset_number == 1:\n",
    "            \n",
    "            if row['ess_0900'] <= 10:\n",
    "                new_column.append('Normal')\n",
    "                \n",
    "            elif row['ess_0900'] > 10:\n",
    "                new_column.append('Sleepy')\n",
    "                \n",
    "        # The more nuanced version - more categories\n",
    "        if dataset_number == 2:\n",
    "            \n",
    "            if row['ess_0900'] <= 5:\n",
    "                new_column.append('Lower Normal')\n",
    "                \n",
    "            elif row['ess_0900'] > 5 and row['ess_0900'] <= 10:\n",
    "                new_column.append('Higher Normal')\n",
    "                \n",
    "            elif row['ess_0900'] > 10 and row['ess_0900'] <= 12:\n",
    "                new_column.append('Mild')\n",
    "                \n",
    "            elif row['ess_0900'] > 12 and row['ess_0900'] <= 15:\n",
    "                new_column.append('Moderate')\n",
    "\n",
    "            elif row['ess_0900'] > 15:\n",
    "                new_column.append('Severe')\n",
    "\n",
    "    # Add the new column to the full df\n",
    "    #ess_categories = pd.DataFrame(new_column)\n",
    "    #df_combined = pd.concat([df, ess_categories], axis=1)\n",
    "\n",
    "    col_index = df.columns.get_loc('ess_0900')\n",
    "    df_copy.insert(col_index+1, 'ess_total_category', new_column)\n",
    "\n",
    "    return df_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb09bb56-bd07-4169-8a11-8cf09a8454ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing columns\n",
    "# It will be too inefficient to try to automate this right now, so I will not write a function (code is just like categorize_0900_scores)\n",
    "# I will manually change the column name, ranges, and category names to replace numbers with the strings of each category\n",
    "\n",
    "def categorize_columns(df):\n",
    "    new_column = []\n",
    "    \n",
    "    # Iterate through the rows of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        # Depending on the range the values fall in, assign categories as strings to a new column\n",
    "                \n",
    "        if row['isi_score'] <= 7:\n",
    "            new_column.append('Not Clinically Significant')\n",
    "    \n",
    "        elif row['isi_score'] > 7 and row['isi_score'] <= 14:\n",
    "            new_column.append('Sub-Threshold Insomnia')\n",
    "            \n",
    "        elif row['isi_score'] > 14 and row['isi_score'] <= 21:\n",
    "            new_column.append('Moderate Insomnia')\n",
    "    \n",
    "        elif row['isi_score'] > 11 and row['isi_score'] <= 28:\n",
    "            new_column.append('Severe Insomnia')\n",
    "    \n",
    "        elif pd.isna(row['isi_score']):\n",
    "            new_column.append(np.nan)\n",
    "    \n",
    "    \n",
    "    # Replace the old column with the new column in the copied dataframe\n",
    "    df['isi_score'] = new_column\n",
    "\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
